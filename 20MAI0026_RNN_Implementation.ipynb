{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20MAI0026_RNN Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqj1ZrPbriHDaU/t1lditn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jency07/Deep-learning-lab/blob/main/20MAI0026_RNN_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNTV0tUMiQGY"
      },
      "source": [
        "***1.Importing libraries:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRKYj7tsgTkn"
      },
      "source": [
        "# Imports\n",
        "%matplotlib notebook\n",
        "\n",
        "import sys\n",
        "import numpy as np  # Matrix and vector computation package\n",
        "import matplotlib\n",
        "\n",
        "np.random.seed(seed=1)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw3N8a3GeYf3"
      },
      "source": [
        "***2.Creating Dataset:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6QKRe2SiYin",
        "outputId": "63ea55d0-5bca-40d2-b7d9-0949e4b4aa94"
      },
      "source": [
        "# Create dataset\n",
        "nb_of_samples = 20\n",
        "sequence_len = 10\n",
        "# Create the sequences\n",
        "X = np.zeros((nb_of_samples, sequence_len))\n",
        "for row_idx in range(nb_of_samples):\n",
        "    X[row_idx,:] = np.around(np.random.rand(sequence_len)).astype(int)\n",
        "# Create the targets for each sequence\n",
        "t = np.sum(X, axis=1)\n",
        "print(\"20MAI0026\")\n",
        "print(\"******************\")\n",
        "print(\"\\n\")\n",
        "print(\"\\nDataset: \\n\")\n",
        "print(X)\n",
        "print(\"\\nTarget: \\n\")\n",
        "print(t)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20MAI0026\n",
            "******************\n",
            "\n",
            "\n",
            "\n",
            "Dataset: \n",
            "\n",
            "[[1. 1. 1. 1. 0. 0. 0. 1. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
            " [0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]\n",
            " [0. 1. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            " [0. 1. 1. 1. 0. 1. 1. 0. 0. 1.]\n",
            " [0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 1. 1. 0. 0. 0. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 0. 1. 1. 0.]\n",
            " [1. 0. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 1. 1. 0. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 1. 1. 0. 0. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
            " [0. 0. 0. 1. 1. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 1. 0. 0. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "Target: \n",
            "\n",
            "[6. 4. 4. 4. 4. 3. 6. 2. 6. 7. 6. 5. 5. 5. 8. 6. 8. 4. 3. 7.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUlU4jBBibzT"
      },
      "source": [
        "***3. RNN Implementation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikWDBUwVicHh"
      },
      "source": [
        "# Define the forward step functions\n",
        "\n",
        "def update_state(xk, sk, wx, wRec):\n",
        "    \"\"\"\n",
        "    Compute state k from the previous state (sk) and current \n",
        "    input (xk), by use of the input weights (wx) and recursive \n",
        "    weights (wRec).\n",
        "    \"\"\"\n",
        "    return xk * wx + sk * wRec\n",
        "\n",
        "\n",
        "def forward_states(X, wx, wRec):\n",
        "    \"\"\"\n",
        "    Unfold the network and compute all state activations \n",
        "    given the input X, input weights (wx), and recursive weights \n",
        "    (wRec). Return the state activations in a matrix, the last \n",
        "    column S[:,-1] contains the final activations.\n",
        "    \"\"\"\n",
        "    # Initialise the matrix that holds all states for all \n",
        "    #  input sequences. The initial state s0 is set to 0.\n",
        "    S = np.zeros((X.shape[0], X.shape[1]+1))\n",
        "    # Use the recurrence relation defined by update_state to update \n",
        "    #  the states trough time.\n",
        "    for k in range(0, X.shape[1]):\n",
        "        # S[k] = S[k-1] * wRec + X[k] * wx\n",
        "        S[:,k+1] = update_state(X[:,k], S[:,k], wx, wRec)\n",
        "    return S\n",
        "\n",
        "\n",
        "def loss(y, t): \n",
        "    \"\"\"MSE between the targets t and the outputs y.\"\"\"\n",
        "    return np.mean((t - y)**2)\n",
        "\n",
        "def output_gradient(y, t):\n",
        "    \"\"\"\n",
        "    Gradient of the MSE loss function with respect to the output y.\n",
        "    \"\"\"\n",
        "    return 2. * (y - t)\n",
        "\n",
        "\n",
        "def backward_gradient(X, S, grad_out, wRec):\n",
        "    \"\"\"\n",
        "    Backpropagate the gradient computed at the output (grad_out) \n",
        "    through the network. Accumulate the parameter gradients for \n",
        "    wX and wRec by for each layer by addition. Return the parameter \n",
        "    gradients as a tuple, and the gradients at the output of each layer.\n",
        "    \"\"\"\n",
        "    # Initialise the array that stores the gradients of the loss with \n",
        "    #  respect to the states.\n",
        "    grad_over_time = np.zeros((X.shape[0], X.shape[1]+1))\n",
        "    grad_over_time[:,-1] = grad_out\n",
        "    # Set the gradient accumulations to 0\n",
        "    wx_grad = 0\n",
        "    wRec_grad = 0\n",
        "    for k in range(X.shape[1], 0, -1):\n",
        "        # Compute the parameter gradients and accumulate the results.\n",
        "        wx_grad += np.sum(\n",
        "            np.mean(grad_over_time[:,k] * X[:,k-1], axis=0))\n",
        "        wRec_grad += np.sum(\n",
        "            np.mean(grad_over_time[:,k] * S[:,k-1]), axis=0)\n",
        "        # Compute the gradient at the output of the previous layer\n",
        "        grad_over_time[:,k-1] = grad_over_time[:,k] * wRec\n",
        "    return (wx_grad, wRec_grad), grad_over_time\n",
        "  \n",
        "  # Define Rprop optimisation function\n",
        "def update_rprop(X, t, W, W_prev_sign, W_delta, eta_p, eta_n):\n",
        "    \"\"\"\n",
        "    Update Rprop values in one iteration.\n",
        "    Args:\n",
        "        X: input data.\n",
        "        t: targets.\n",
        "        W: Current weight parameters.\n",
        "        W_prev_sign: Previous sign of the W gradient.\n",
        "        W_delta: Rprop update values (Delta).\n",
        "        eta_p, eta_n: Rprop hyperparameters.\n",
        "    Returns:\n",
        "        (W_delta, W_sign): Weight update and sign of last weight\n",
        "                           gradient.\n",
        "    \"\"\"\n",
        "    # Perform forward and backward pass to get the gradients\n",
        "    S = forward_states(X, W[0], W[1])\n",
        "    grad_out = output_gradient(S[:,-1], t)\n",
        "    W_grads, _ = backward_gradient(X, S, grad_out, W[1])\n",
        "    W_sign = np.sign(W_grads)  # Sign of new gradient\n",
        "    # Update the Delta (update value) for each weight \n",
        "    #  parameter seperately\n",
        "    for i, _ in enumerate(W):\n",
        "        if W_sign[i] == W_prev_sign[i]:\n",
        "            W_delta[i] *= eta_p\n",
        "        else:\n",
        "            W_delta[i] *= eta_n\n",
        "    return W_delta, W_sign"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFx4ONrLkebz"
      },
      "source": [
        "***4.Training the model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmffRkP0iubO",
        "outputId": "75db5ee3-881a-4514-b616-1292ddf114a2"
      },
      "source": [
        "\n",
        "print(\"20MAI0026\")\n",
        "print(\"******************\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Perform gradient checking\n",
        "# Set the weight parameters used during gradient checking\n",
        "params = [1.2, 1.2]  # [wx, wRec]\n",
        "# Set the small change to compute the numerical gradient\n",
        "eps = 1e-7\n",
        "# Compute the backprop gradients\n",
        "S = forward_states(X, params[0], params[1])\n",
        "grad_out = output_gradient(S[:,-1], t)\n",
        "backprop_grads, grad_over_time = backward_gradient(\n",
        "    X, S, grad_out, params[1])\n",
        "# Compute the numerical gradient for each parameter in the layer\n",
        "for p_idx, _ in enumerate(params):\n",
        "    grad_backprop = backprop_grads[p_idx]\n",
        "    # + eps\n",
        "    params[p_idx] += eps\n",
        "    plus_loss = loss(forward_states(X, params[0], params[1])[:,-1], t)\n",
        "    # - eps\n",
        "    params[p_idx] -= 2 * eps\n",
        "    min_loss = loss(forward_states(X, params[0], params[1])[:,-1], t)\n",
        "    # reset param value\n",
        "    params[p_idx] += eps\n",
        "    # calculate numerical gradient\n",
        "    grad_num = (plus_loss - min_loss) / (2*eps)\n",
        "    # Raise error if the numerical grade is not close to \n",
        "    #  the backprop gradient\n",
        "    if not np.isclose(grad_num, grad_backprop):\n",
        "        raise ValueError((\n",
        "            f'Numerical gradient of {grad_num:.6f} is not close to '\n",
        "            f'the backpropagation gradient of {grad_backprop:.6f}!'))\n",
        "print('No gradient errors found')\n",
        "\n",
        "# Perform Rprop optimisation\n",
        "\n",
        "# Set hyperparameters\n",
        "eta_p = 1.2\n",
        "eta_n = 0.5\n",
        "\n",
        "# Set initial parameters\n",
        "W = [-1.5, 2]  # [wx, wRec]\n",
        "W_delta = [0.001, 0.001]  # Update values (Delta) for W\n",
        "W_sign = [0, 0]  # Previous sign of W\n",
        "\n",
        "ls_of_ws = [(W[0], W[1])]  # List of weights to plot\n",
        "# Iterate over 500 iterations\n",
        "for i in range(500):\n",
        "    # Get the update values and sign of the last gradient\n",
        "    W_delta, W_sign = update_rprop(\n",
        "        X, t, W, W_sign, W_delta, eta_p, eta_n)\n",
        "    # Update each weight parameter seperately\n",
        "    for i, _ in enumerate(W):\n",
        "        W[i] -= W_sign[i] * W_delta[i]\n",
        "    ls_of_ws.append((W[0], W[1]))  # Add weights to list to plot\n",
        "\n",
        "print(f'Final weights are: wx = {W[0]:.4f},  wRec = {W[1]:.4f}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20MAI0026\n",
            "******************\n",
            "\n",
            "\n",
            "No gradient errors found\n",
            "Final weights are: wx = 1.0009,  wRec = 0.9998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7PmqArjkh-c"
      },
      "source": [
        "***5.Testing the model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zOEjGbhjBvm",
        "outputId": "6eb46c72-a1a4-4ab5-af2e-d00f0a0dabbf"
      },
      "source": [
        "\n",
        "print(\"20MAI0026\")\n",
        "print(\"******************\")\n",
        "print(\"\\n\")\n",
        "\n",
        "test_inpt = np.asmatrix([[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1]])\n",
        "test_outpt = forward_states(test_inpt, W[0], W[1])[:,-1]\n",
        "sum_test_inpt = test_inpt.sum()\n",
        "print((\n",
        "    f'Target output: {sum_test_inpt:d} vs Model output: '\n",
        "    f'{test_outpt[0]:.2f}'))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20MAI0026\n",
            "******************\n",
            "\n",
            "\n",
            "Target output: 5 vs Model output: 5.00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}